# Dobi - SVD Installation and Testing

## Insatalling the repo and the environment
/Users/alessandromason/Downloads/notes.md
```python
git clone https://github.com/wangqinsi1/Dobi-SVD.git
cd Dobi-SVD
```

```python
conda create -yn dobisvd python=3.10
conda activate dobisvd
pip install -e .
```

## Downloading the pre-trained model from hugging-face

```python
git clone https://huggingface.co/Qinsi1/DobiSVD-Llama-2-7b-hf-0.4 ./results/compressed_model/Llama-2-7b-hf/DobiSVD-Llama-2-7b-hf-0.4
```
This dosnt actually download the model, it downloads a git reference to it. to dowload the two models referenced in quick_run.py directly from hugging face use:
```
python -c "from huggingface_hub import snapshot_download; snapshot_download('Qinsi1/DobiSVD-Llama-2-7b-hf-0.4', local_dir='./results/compressed_model/Llama-2-7b-hf/DobiSVD-Llama-2-7b-hf-0.4', local_dir_use_symlinks=False)"

python -c "from huggingface_hub import snapshot_download; snapshot_download('Qinsi1/DobiSVD_Noremapping-Llama-2-7b-hf-0.8', local_dir='./results/compressed_model/Llama-2-7b-hf/DobiSVD_Noremapping-Llama-2-7b-hf-0.8', local_dir_use_symlinks=False)"
```

## Testing the model
To test the model create a file called quick_run.py in the main Dobi-SVD directory with the following content

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from modelutils import load_remapping_model, load_unremapping_model

# Load the remapping Model
model_id = "./results/compressed_model/Llama-2-7b-hf/DobiSVD-Llama-2-7b-hf-0.4"
model, tokenizer = load_remapping_model(model_id)

# Load the unremapping Model
model_id = "./results/compressed_model/Llama-2-7b-hf/DobiSVD_Noremapping-Llama-2-7b-hf-0.8"
model, tokenizer = load_unremapping_model(model_id)


# Generate Answer (Model usage is exactly the same as Llama)
model = model.cuda()
prompt = "What is the responsibility of the AI assistant?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=50)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

## Setup HPC environment

```
srun -p gpu -A vxc204 -c 4 --mem=32GB --gpus=1 --pty bash

conda activate dobisvd

module load CUDA/12.1
module load GCC/10.3
```
## Running the model 

```python qick_run.py ```

## Memory constraints

```
`orch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 11.90 GiB of which 22.62 MiB is free. Including non-PyTorch memory, this process has 11.88 GiB memory in use. Of the allocated memory 10.67 GiB is allocated by PyTorch, and 944.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CO
```
## New Script
```

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from modelutils import load_remapping_model, load_unremapping_model

# Clear GPU memory before starting
torch.cuda.empty_cache()

# Set memory management options
torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of GPU memory
torch.cuda.empty_cache()

print("Loading model...")

# Load the remapping Model
model_id = "./results/compressed_model/Llama-2-7b-hf/DobiSVD-Llama-2-7b-hf-0.4"
model, tokenizer = load_remapping_model(model_id)

# Clear memory after loading first model
torch.cuda.empty_cache()

# Load the unremapping Model
model_id = "./results/compressed_model/Llama-2-7b-hf/DobiSVD_Noremapping-Llama-2-7b-hf-0.8"
model, tokenizer = load_unremapping_model(model_id)

# Clear memory after loading second model
torch.cuda.empty_cache()

print("Model loaded successfully!")

# Generate Answer with memory optimization
model = model.cuda()
prompt = "What is the responsibility of the AI assistant?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Use memory-efficient generation settings
with torch.cuda.amp.autocast():  # Use mixed precision
    outputs = model.generate(
        **inputs, 
        max_new_tokens=50,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        use_cache=True
    )

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)

# Clear memory after generation
torch.cuda.empty_cache()
```
# Still have VRAM issues.
the model is trying to load 50 MiB in the GPU but the P100 we had allocated had max 12G. so we searched for GPU's with higher allocation

```
sinfo -o "%P %a %l %G %F"              # partition, avail, time-limit, GRES, features
sinfo -N -p gpu -o "%N %t %G %f %m"    # per-node: name, state, GRES, features, RAM

# 2) Sometimes features list memory size tags (e.g., a100_80g, v100_32g)
# If unsure, dump full node records:
scontrol show nodes | egrep -i "NodeName=|Gres=|Features=|RealMemory=" -n
```

this allowed us to target firstly an H100 (80GB) but we coun't get it allocated so switched to a L40S (48GB)

```
srun -p gpu -A vxc204 -c 4 --mem=32G --gres=gpu:1 --constraint=gpul40s --pty bash
```

# Resoults

This finally allowed for the model to run and gave the following output

```
python quick_run.py    
Dequantize the model after remaping.: 454it [00:02, 189.96it/s]
What is the responsibility of the AI assistant? The AI assistant is the assistant of the AI assistant. The AI assistant is the AI assistant of the AI assistant. The AI assistant is the AI assistant of the AI assistant. The AI assistant is the A
```

